{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Swiler's file paths for each dataset. \n",
    "gdp_data = pd.read_csv('/Users/swilerboyd/Documents/GitHub/cs526_proj/datasets/quarterly/GDP.csv', parse_dates=['DATE'])\n",
    "cpat_tax = pd.read_csv('/Users/swilerboyd/Documents/GitHub/cs526_proj/datasets/quarterly/CPATAX.csv', parse_dates=['DATE'])\n",
    "durable_goods = pd.read_csv('/Users/swilerboyd/Documents/GitHub/cs526_proj/datasets/quarterly/manufacturers_new_orders_durable_goods_excluding_defense.csv', parse_dates=['DATE'])\n",
    "housing_starts = pd.read_csv('/Users/swilerboyd/Documents/GitHub/cs526_proj/datasets/quarterly/housing start.csv', parse_dates=['DATE'])\n",
    "industrial_production = pd.read_csv('/Users/swilerboyd/Documents/GitHub/cs526_proj/datasets/quarterly/industrial_production_total_index.csv', parse_dates=['DATE'])\n",
    "personal_consumption = pd.read_csv('/Users/swilerboyd/Documents/GitHub/cs526_proj/datasets/quarterly/PCECTPI.csv', parse_dates=['DATE'])\n",
    "t10y2y = pd.read_csv('/Users/swilerboyd/Documents/GitHub/cs526_proj/datasets/quarterly/T10Y2Y.csv', parse_dates=['DATE'])\n",
    "sp500_vix = pd.read_csv('/Users/swilerboyd/Documents/GitHub/cs526_proj/datasets/quarterly/sp500_vix_quarterly.csv', parse_dates=['Date'])\n",
    "gtrend_recession = pd.read_csv('/Users/swilerboyd/Documents/GitHub/cs526_proj/datasets/quarterly/gtrend_recession.csv', parse_dates=['Month'])\n",
    "gtrend_unemployment = pd.read_csv('/Users/swilerboyd/Documents/GitHub/cs526_proj/datasets/quarterly/gtrend_unemployment.csv', parse_dates=['Month'])\n",
    "nonderiv_insider_activity = pd.read_csv('/Users/swilerboyd/Documents/GitHub/cs526_proj/datasets/quarterly/nonderiv_insider_activity.csv', parse_dates=['transactionDate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Henry's file paths for each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        DATE      GDP  CPATAX  ADXDNO  HOUST  IPB50001SQ  PCECTPI  T10Y2Y  \\\n",
      "0 1947-01-01  243.164   9.959     NaN    NaN     13.7361   11.557     NaN   \n",
      "1 1947-04-01  245.968  13.603     NaN    NaN     13.7450   11.649     NaN   \n",
      "2 1947-07-01  249.585  13.868     NaN    NaN     13.7719   11.866     NaN   \n",
      "3 1947-10-01  259.745  14.455     NaN    NaN     14.1482   12.162     NaN   \n",
      "4 1948-01-01  265.742  17.971     NaN    NaN     14.2916   12.297     NaN   \n",
      "\n",
      "   S&P 500  VIX  recession: (United States)  \\\n",
      "0      NaN  NaN                         NaN   \n",
      "1      NaN  NaN                         NaN   \n",
      "2      NaN  NaN                         NaN   \n",
      "3      NaN  NaN                         NaN   \n",
      "4      NaN  NaN                         NaN   \n",
      "\n",
      "   Unemployment benefits: (United States)  net_buys  \n",
      "0                                     NaN       NaN  \n",
      "1                                     NaN       NaN  \n",
      "2                                     NaN       NaN  \n",
      "3                                     NaN       NaN  \n",
      "4                                     NaN       NaN  \n"
     ]
    }
   ],
   "source": [
    "# Merge datasets on their respective date columns\n",
    "data = gdp_data.copy()  # Start with GDP data\n",
    "\n",
    "# Merge other datasets using 'DATE' or appropriate columns\n",
    "data = data.merge(cpat_tax, on='DATE', how='left')\n",
    "data = data.merge(durable_goods, on='DATE', how='left')\n",
    "data = data.merge(housing_starts, on='DATE', how='left')\n",
    "data = data.merge(industrial_production, on='DATE', how='left')\n",
    "data = data.merge(personal_consumption, on='DATE', how='left')\n",
    "data = data.merge(t10y2y, on='DATE', how='left')\n",
    "data = data.merge(sp500_vix.rename(columns={'Date': 'DATE'}), on='DATE', how='left')\n",
    "data = data.merge(gtrend_recession.rename(columns={'Month': 'DATE'}), on='DATE', how='left')\n",
    "data = data.merge(gtrend_unemployment.rename(columns={'Month': 'DATE'}), on='DATE', how='left')\n",
    "data = data.merge(nonderiv_insider_activity.rename(columns={'transactionDate': 'DATE'}), on='DATE', how='left')\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a recession indicator\n",
    "data['GDP_diff'] = data['GDP'].diff()\n",
    "data['Recession'] = np.where(data['GDP_diff'] < 0, 1, 0)\n",
    "\n",
    "# Count the total number of recession periods\n",
    "recession_count = data['Recession'].sum()\n",
    "\n",
    "# Filter the dataset for recession periods\n",
    "recession_periods = data[data['Recession'] == 1][['DATE', 'GDP']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     1947-01-01\n",
      "1     1947-04-01\n",
      "2     1947-07-01\n",
      "3     1947-10-01\n",
      "4     1948-01-01\n",
      "         ...    \n",
      "305   2023-04-01\n",
      "306   2023-07-01\n",
      "307   2023-10-01\n",
      "308   2024-01-01\n",
      "309   2024-04-01\n",
      "Name: DATE, Length: 310, dtype: datetime64[ns]\n",
      "Stratified k-Fold Cross-Validation Accuracy Scores: [0.62903226 0.58064516 0.62903226 0.62903226 0.53225806]\n",
      "Mean Accuracy: 0.6\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for modeling\n",
    "# This model uses the variables that are complete 1947-2023\n",
    "X = data[['CPATAX', 'PCECTPI', 'IPB50001SQ']].dropna()\n",
    "y = data['Recession'].loc[X.index]  # Ensure y matches the non-NaN rows of X\n",
    "\n",
    "# Get the dates corresponding to the non-NaN rows in X\n",
    "dates = data.loc[X.index, 'DATE']  # Assuming 'DATE' is the column for the dates in your dataset\n",
    "\n",
    "print(dates)\n",
    "\n",
    "# Scale the features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Define the model (you can use LogisticRegression or RandomForest)\n",
    "model = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "\n",
    "# Initialize Stratified k-fold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Cross-validate using StratifiedKFold\n",
    "cv_scores = cross_val_score(model, X_scaled, y, cv=skf, scoring='accuracy')\n",
    "\n",
    "print('Stratified k-Fold Cross-Validation Accuracy Scores:', cv_scores)\n",
    "print('Mean Accuracy:', np.mean(cv_scores))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180   1992-01-01\n",
      "181   1992-04-01\n",
      "182   1992-07-01\n",
      "183   1992-10-01\n",
      "184   1993-01-01\n",
      "         ...    \n",
      "305   2023-04-01\n",
      "306   2023-07-01\n",
      "307   2023-10-01\n",
      "308   2024-01-01\n",
      "309   2024-04-01\n",
      "Name: DATE, Length: 130, dtype: datetime64[ns]\n",
      "Stratified k-Fold Cross-Validation Accuracy Scores: [0.80769231 0.92307692 0.88461538 0.88461538 0.80769231]\n",
      "Mean Accuracy: 0.8615384615384615\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for modeling\n",
    "X = data[['CPATAX', 'PCECTPI', 'IPB50001SQ', 'HOUST', 'ADXDNO', 'T10Y2Y', 'S&P 500']].dropna()\n",
    "y = data['Recession'].loc[X.index]  # Ensure y matches the non-NaN rows of X\n",
    "\n",
    "# Get the dates corresponding to the non-NaN rows in X\n",
    "dates = data.loc[X.index, 'DATE']  # Assuming 'DATE' is the column for the dates in your dataset\n",
    "print(dates)\n",
    "\n",
    "# Scale the features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Define the model (you can use LogisticRegression or RandomForest)\n",
    "model = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "\n",
    "# Initialize Stratified k-fold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Cross-validate using StratifiedKFold\n",
    "cv_scores = cross_val_score(model, X_scaled, y, cv=skf, scoring='accuracy')\n",
    "\n",
    "print('Stratified k-Fold Cross-Validation Accuracy Scores:', cv_scores)\n",
    "print('Mean Accuracy:', np.mean(cv_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228   2004-01-01\n",
      "229   2004-04-01\n",
      "230   2004-07-01\n",
      "231   2004-10-01\n",
      "232   2005-01-01\n",
      "         ...    \n",
      "301   2022-04-01\n",
      "302   2022-07-01\n",
      "303   2022-10-01\n",
      "304   2023-01-01\n",
      "305   2023-04-01\n",
      "Name: DATE, Length: 78, dtype: datetime64[ns]\n",
      "Stratified k-Fold Cross-Validation Accuracy Scores: [1.         0.8125     0.8125     0.8        0.93333333]\n",
      "Mean Accuracy: 0.8716666666666667\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for modeling\n",
    "X = data[['CPATAX', 'PCECTPI', 'IPB50001SQ', 'HOUST', 'ADXDNO', 'T10Y2Y','S&P 500', 'VIX', \n",
    "          'recession: (United States)', 'Unemployment benefits: (United States)', 'net_buys']].dropna()\n",
    "y = data['Recession'].loc[X.index]  # Ensure y matches the non-NaN rows of X\n",
    "\n",
    "# Get the dates corresponding to the non-NaN rows in X\n",
    "dates = data.loc[X.index, 'DATE']  # Assuming 'DATE' is the column for the dates in your dataset\n",
    "\n",
    "print(dates)\n",
    "\n",
    "# Scale the features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Define the model (you can use LogisticRegression or RandomForest)\n",
    "model = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "\n",
    "# Initialize Stratified k-fold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Cross-validate using StratifiedKFold\n",
    "cv_scores = cross_val_score(model, X_scaled, y, cv=skf, scoring='accuracy')\n",
    "\n",
    "print('Stratified k-Fold Cross-Validation Accuracy Scores:', cv_scores)\n",
    "print('Mean Accuracy:', np.mean(cv_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
