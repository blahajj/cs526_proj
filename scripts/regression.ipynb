{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Swiler's file paths for each dataset. \n",
    "gdp_data = pd.read_csv('/Users/swilerboyd/Documents/GitHub/cs526_proj/datasets/quarterly/GDP.csv', parse_dates=['DATE'])\n",
    "cpat_tax = pd.read_csv('/Users/swilerboyd/Documents/GitHub/cs526_proj/datasets/quarterly/CPATAX.csv', parse_dates=['DATE'])\n",
    "durable_goods = pd.read_csv('/Users/swilerboyd/Documents/GitHub/cs526_proj/datasets/quarterly/manufacturers_new_orders_durable_goods_excluding_defense.csv', parse_dates=['DATE'])\n",
    "housing_starts = pd.read_csv('/Users/swilerboyd/Documents/GitHub/cs526_proj/datasets/quarterly/housing start.csv', parse_dates=['DATE'])\n",
    "industrial_production = pd.read_csv('/Users/swilerboyd/Documents/GitHub/cs526_proj/datasets/quarterly/industrial_production_total_index.csv', parse_dates=['DATE'])\n",
    "personal_consumption = pd.read_csv('/Users/swilerboyd/Documents/GitHub/cs526_proj/datasets/quarterly/PCECTPI.csv', parse_dates=['DATE'])\n",
    "t10y2y = pd.read_csv('/Users/swilerboyd/Documents/GitHub/cs526_proj/datasets/quarterly/T10Y2Y.csv', parse_dates=['DATE'])\n",
    "sp500_vix = pd.read_csv('/Users/swilerboyd/Documents/GitHub/cs526_proj/datasets/quarterly/sp500_vix_quarterly.csv', parse_dates=['Date'])\n",
    "gtrend_recession = pd.read_csv('/Users/swilerboyd/Documents/GitHub/cs526_proj/datasets/quarterly/gtrend_recession.csv', parse_dates=['Month'])\n",
    "gtrend_unemployment = pd.read_csv('/Users/swilerboyd/Documents/GitHub/cs526_proj/datasets/quarterly/gtrend_unemployment.csv', parse_dates=['Month'])\n",
    "nonderiv_insider_activity = pd.read_csv('/Users/swilerboyd/Documents/GitHub/cs526_proj/datasets/quarterly/nonderiv_insider_activity.csv', parse_dates=['transactionDate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Henry's file paths for each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        DATE      GDP  CPATAX  ADXDNO  HOUST  IPB50001SQ  PCECTPI  T10Y2Y  \\\n",
      "0 1947-01-01  243.164   9.959     NaN    NaN     13.7361   11.557     NaN   \n",
      "1 1947-04-01  245.968  13.603     NaN    NaN     13.7450   11.649     NaN   \n",
      "2 1947-07-01  249.585  13.868     NaN    NaN     13.7719   11.866     NaN   \n",
      "3 1947-10-01  259.745  14.455     NaN    NaN     14.1482   12.162     NaN   \n",
      "4 1948-01-01  265.742  17.971     NaN    NaN     14.2916   12.297     NaN   \n",
      "\n",
      "   S&P 500  VIX  recession: (United States)  \\\n",
      "0      NaN  NaN                         NaN   \n",
      "1      NaN  NaN                         NaN   \n",
      "2      NaN  NaN                         NaN   \n",
      "3      NaN  NaN                         NaN   \n",
      "4      NaN  NaN                         NaN   \n",
      "\n",
      "   Unemployment benefits: (United States)  net_buys  \n",
      "0                                     NaN       NaN  \n",
      "1                                     NaN       NaN  \n",
      "2                                     NaN       NaN  \n",
      "3                                     NaN       NaN  \n",
      "4                                     NaN       NaN  \n"
     ]
    }
   ],
   "source": [
    "# Merge datasets on their respective date columns\n",
    "data = gdp_data.copy()  # Start with GDP data\n",
    "\n",
    "# Merge other datasets using 'DATE' or appropriate columns\n",
    "data = data.merge(cpat_tax, on='DATE', how='left')\n",
    "data = data.merge(durable_goods, on='DATE', how='left')\n",
    "data = data.merge(housing_starts, on='DATE', how='left')\n",
    "data = data.merge(industrial_production, on='DATE', how='left')\n",
    "data = data.merge(personal_consumption, on='DATE', how='left')\n",
    "data = data.merge(t10y2y, on='DATE', how='left')\n",
    "data = data.merge(sp500_vix.rename(columns={'Date': 'DATE'}), on='DATE', how='left')\n",
    "data = data.merge(gtrend_recession.rename(columns={'Month': 'DATE'}), on='DATE', how='left')\n",
    "data = data.merge(gtrend_unemployment.rename(columns={'Month': 'DATE'}), on='DATE', how='left')\n",
    "data = data.merge(nonderiv_insider_activity.rename(columns={'transactionDate': 'DATE'}), on='DATE', how='left')\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a recession indicator based on a decline in GDP from one quarter to the next\n",
    "data['GDP_diff'] = data['GDP'].diff()\n",
    "data['Recession'] = np.where(data['GDP_diff'] < 0, 1, 0)\n",
    "\n",
    "# Shift 'Recession' column by -1 to predict recession in the next quarter\n",
    "data['Recession_next_quarter'] = data['Recession'].shift(-1)\n",
    "\n",
    "# Drop rows with missing target values (for the last row where we can't predict the next quarter)\n",
    "data.dropna(subset=['Recession_next_quarter'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     1947-01-01\n",
      "1     1947-04-01\n",
      "2     1947-07-01\n",
      "3     1947-10-01\n",
      "4     1948-01-01\n",
      "         ...    \n",
      "304   2023-01-01\n",
      "305   2023-04-01\n",
      "306   2023-07-01\n",
      "307   2023-10-01\n",
      "308   2024-01-01\n",
      "Name: DATE, Length: 309, dtype: datetime64[ns]\n",
      "180   1992-01-01\n",
      "181   1992-04-01\n",
      "182   1992-07-01\n",
      "183   1992-10-01\n",
      "184   1993-01-01\n",
      "         ...    \n",
      "304   2023-01-01\n",
      "305   2023-04-01\n",
      "306   2023-07-01\n",
      "307   2023-10-01\n",
      "308   2024-01-01\n",
      "Name: DATE, Length: 129, dtype: datetime64[ns]\n",
      "228   2004-01-01\n",
      "229   2004-04-01\n",
      "230   2004-07-01\n",
      "231   2004-10-01\n",
      "232   2005-01-01\n",
      "         ...    \n",
      "301   2022-04-01\n",
      "302   2022-07-01\n",
      "303   2022-10-01\n",
      "304   2023-01-01\n",
      "305   2023-04-01\n",
      "Name: DATE, Length: 78, dtype: datetime64[ns]\n",
      "\n",
      "Logistic Regression Model 1 Report (Test Set):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.51      0.67        89\n",
      "         1.0       0.06      0.75      0.12         4\n",
      "\n",
      "    accuracy                           0.52        93\n",
      "   macro avg       0.52      0.63      0.39        93\n",
      "weighted avg       0.94      0.52      0.64        93\n",
      "\n",
      "\n",
      "Logistic Regression Model 2 Report (Test Set):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.86      0.91        37\n",
      "         1.0       0.17      0.50      0.25         2\n",
      "\n",
      "    accuracy                           0.85        39\n",
      "   macro avg       0.57      0.68      0.58        39\n",
      "weighted avg       0.93      0.85      0.88        39\n",
      "\n",
      "\n",
      "Logistic Regression Model 3 Report (Test Set):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.96      0.98        23\n",
      "         1.0       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.96        24\n",
      "   macro avg       0.75      0.98      0.82        24\n",
      "weighted avg       0.98      0.96      0.96        24\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the three different versions of X with different sets of variables\n",
    "X_model1 = data[['CPATAX', 'PCECTPI', 'IPB50001SQ']]  # Variables for model 1\n",
    "X_model2 = data[['CPATAX', 'PCECTPI', 'IPB50001SQ', 'HOUST', 'ADXDNO', 'T10Y2Y', 'S&P 500']].dropna()  # Variables for model 2\n",
    "X_model3 = data[['CPATAX', 'PCECTPI', 'IPB50001SQ', 'HOUST', 'ADXDNO', 'T10Y2Y', 'S&P 500', 'VIX', \n",
    "          'recession: (United States)', 'Unemployment benefits: (United States)', 'net_buys']].dropna()  # Variables for model 3\n",
    "\n",
    "# Target variable remains the same\n",
    "y = data['Recession_next_quarter']\n",
    "\n",
    "# Drop rows with missing values from X and y for each model to ensure consistency\n",
    "\n",
    "# Model 1\n",
    "Xy_model1 = pd.concat([X_model1, y], axis=1).dropna()\n",
    "X_model1_cleaned = Xy_model1.drop(columns=['Recession_next_quarter'])\n",
    "y_model1_cleaned = Xy_model1['Recession_next_quarter']\n",
    "# Get the dates corresponding to the non-NaN rows in X\n",
    "dates = data.loc[Xy_model1.index, 'DATE']  # Assuming 'DATE' is the column for the dates in your dataset\n",
    "print(dates)\n",
    "\n",
    "# Model 2\n",
    "Xy_model2 = pd.concat([X_model2, y], axis=1).dropna()\n",
    "X_model2_cleaned = Xy_model2.drop(columns=['Recession_next_quarter'])\n",
    "y_model2_cleaned = Xy_model2['Recession_next_quarter']\n",
    "# Get the dates corresponding to the non-NaN rows in X\n",
    "dates = data.loc[Xy_model2.index, 'DATE']  # Assuming 'DATE' is the column for the dates in your dataset\n",
    "print(dates)\n",
    "\n",
    "# Model 3\n",
    "Xy_model3 = pd.concat([X_model3, y], axis=1).dropna()\n",
    "X_model3_cleaned = Xy_model3.drop(columns=['Recession_next_quarter'])\n",
    "y_model3_cleaned = Xy_model3['Recession_next_quarter']\n",
    "# Get the dates corresponding to the non-NaN rows in X\n",
    "dates = data.loc[Xy_model3.index, 'DATE']  # Assuming 'DATE' is the column for the dates in your dataset\n",
    "print(dates)\n",
    "\n",
    "# Scale the features for each model\n",
    "scaler = StandardScaler()\n",
    "X_model1_scaled = scaler.fit_transform(X_model1_cleaned)\n",
    "X_model2_scaled = scaler.fit_transform(X_model2_cleaned)\n",
    "X_model3_scaled = scaler.fit_transform(X_model3_cleaned)\n",
    "\n",
    "# Split the data into train and test sets for each model (70% train, 30% test)\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X_model1_scaled, y_model1_cleaned, test_size=0.3, random_state=42)\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X_model2_scaled, y_model2_cleaned, test_size=0.3, random_state=42)\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(X_model3_scaled, y_model3_cleaned, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train and predict using Logistic Regression for each model\n",
    "logreg = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "\n",
    "# Model 1\n",
    "logreg.fit(X_train1, y_train1)\n",
    "y_pred_model1 = logreg.predict(X_test1)\n",
    "\n",
    "# Model 2\n",
    "logreg.fit(X_train2, y_train2)\n",
    "y_pred_model2 = logreg.predict(X_test2)\n",
    "\n",
    "# Model 3\n",
    "logreg.fit(X_train3, y_train3)\n",
    "y_pred_model3 = logreg.predict(X_test3)\n",
    "\n",
    "# Model 1 evaluation\n",
    "print(\"\\nLogistic Regression Model 1 Report (Test Set):\\n\", classification_report(y_test1, y_pred_model1, zero_division=0))\n",
    "\n",
    "# Model 2 evaluation\n",
    "print(\"\\nLogistic Regression Model 2 Report (Test Set):\\n\", classification_report(y_test2, y_pred_model2, zero_division=0))\n",
    "\n",
    "# Model 3 evaluation\n",
    "print(\"\\nLogistic Regression Model 3 Report (Test Set):\\n\", classification_report(y_test3, y_pred_model3, zero_division=0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
